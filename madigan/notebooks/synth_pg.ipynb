{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from madigan.environments import make_env\n",
    "from madigan.environments.cpp import Broker, Synth, Env as EnvC\n",
    "from madigan.environments.cpp import Assets, RiskInfo, EnvInfoMulti, EnvInfoSingle\n",
    "\n",
    "from madigan.fleet import make_agent\n",
    "\n",
    "from madigan.utils.preprocessor import make_preprocessor as _make_preprocessor\n",
    "from madigan.utils import make_config, State\n",
    "from madigan.utils import ReplayBuffer, SARSD, DiscreteActionSpace\n",
    "from madigan.utils import list_2_dict, reduce_train_metrics\n",
    "\n",
    "\n",
    "from madigan.run.test import test\n",
    "from madigan.utils.plotting import plot_test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basepath': '/media/hemu/Data/Markets/farm',\n",
       " 'experiment_id': 'SineQ',\n",
       " 'parent_id': '',\n",
       " 'overwrite_exp': False,\n",
       " 'transaction_cost_abs': 0.0,\n",
       " 'transaction_cost_rel': 0.01,\n",
       " 'slippage_abs': 0.0,\n",
       " 'slippage_rel': 0.0,\n",
       " 'env_type': 'Synth',\n",
       " 'data_source_type': 'SineAdder',\n",
       " 'init_cash': 1000000,\n",
       " 'required_margin': 1.0,\n",
       " 'maintenance_margin': 0.25,\n",
       " 'generator_params': {'freq': [2.2, 4.1, 1.0, 3.0],\n",
       "  'mu': [0.6, 0.3, 2.0, 4.2],\n",
       "  'amp': [0.5, 0.2, 0.4, 1.2],\n",
       "  'phase': [0.0, 1.0, 4.0, 0.0],\n",
       "  'dX': 0.01,\n",
       "  'noise': 0.0},\n",
       " 'assets': ['OU1'],\n",
       " 'lot_unit_value': 100000,\n",
       " 'n_assets': 1,\n",
       " 'discrete_actions': True,\n",
       " 'discrete_action_atoms': 3,\n",
       " 'preprocessor_type': 'WindowedStacker',\n",
       " 'preprocessor_config': {'window_length': 64},\n",
       " 'agent_type': 'DQN',\n",
       " 'agent_config': {'type': 'DQN',\n",
       "  'basepath': '/media/hemu/Data/Markets/farm',\n",
       "  'discrete_action_atoms': 3,\n",
       "  'model_config': {'model_class': 'ConvModel',\n",
       "   'd_model': 256,\n",
       "   'n_layers': 4,\n",
       "   'n_feats': 1,\n",
       "   'action_atoms': 3,\n",
       "   'n_assets': 1,\n",
       "   'min_tf': 64,\n",
       "   'dueling': False,\n",
       "   'iqn': False,\n",
       "   'nTau1': 32,\n",
       "   'nTau2': 32,\n",
       "   'tau_embed_size': 64,\n",
       "   'discrete_actions': True,\n",
       "   'discrete_action_atoms': 3,\n",
       "   'lot_unit_value': 100000},\n",
       "  'optim_config': {'type': 'Adam',\n",
       "   'lr': 0.001,\n",
       "   'lr_critic': 0.001,\n",
       "   'lr_actor': 0.0001,\n",
       "   'eps': 1e-08,\n",
       "   'momentum': 0.9,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'weight_decay': 0},\n",
       "  'double_dqn': True,\n",
       "  'dueling': False,\n",
       "  'iqn': False,\n",
       "  'nTau1': 32,\n",
       "  'nTau2': 32,\n",
       "  'k_huber': 1.0,\n",
       "  'tau_embed_size': 64,\n",
       "  'discount': 0.999,\n",
       "  'nstep_return': 3,\n",
       "  'action_atoms': 3,\n",
       "  'tau_soft_update': 0.0001,\n",
       "  'greedy_eps_testing': 0.0},\n",
       " 'model_config': {'model_class': 'ConvModel',\n",
       "  'd_model': 256,\n",
       "  'n_layers': 4,\n",
       "  'n_feats': 1,\n",
       "  'action_atoms': 3,\n",
       "  'n_assets': 1,\n",
       "  'min_tf': 64,\n",
       "  'dueling': False,\n",
       "  'iqn': False,\n",
       "  'nTau1': 32,\n",
       "  'nTau2': 32,\n",
       "  'tau_embed_size': 64,\n",
       "  'discrete_actions': True,\n",
       "  'discrete_action_atoms': 3,\n",
       "  'lot_unit_value': 100000},\n",
       " 'optim_config': {'type': 'Adam',\n",
       "  'lr': 0.001,\n",
       "  'lr_critic': 0.001,\n",
       "  'lr_actor': 0.0001,\n",
       "  'eps': 1e-08,\n",
       "  'momentum': 0.9,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'weight_decay': 0},\n",
       " 'nsteps': 1000000,\n",
       " 'test_steps': 1000,\n",
       " 'rb_size': 100000,\n",
       " 'episode_length': 1024,\n",
       " 'min_rb_size': 50000,\n",
       " 'train_freq': 4,\n",
       " 'target_update_freq': 12000,\n",
       " 'test_freq': 32000,\n",
       " 'log_freq': 10000,\n",
       " 'model_save_freq': 64000,\n",
       " 'min_tf': 64,\n",
       " 'batch_size': 32,\n",
       " 'nstep_return': 3,\n",
       " 'expl_eps': 1.0,\n",
       " 'expl_eps_min': 0.1,\n",
       " 'expl_eps_decay': 1e-06,\n",
       " 'reward_clip': (-1.0, 1.0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config = make_config(\n",
    "        experiment_id=\"SineQ\",\n",
    "        basepath=\"/media/hemu/Data/Markets/farm\",\n",
    "        overwrite_exp=False,\n",
    "        test_steps=1_000,\n",
    "        nsteps=1_000_000,\n",
    "        transaction_cost_rel=0.01,\n",
    "    \n",
    "        assets=[\"OU1\"],\n",
    "        data_source_type=\"SineAdder\",\n",
    "        generator_params={\n",
    "            'freq':[2.2, 4.1, 1., 3.],\n",
    "            'mu':[.6, 0.3, 2., 4.2],\n",
    "            'amp':[.5, 0.2, 0.4, 1.2],\n",
    "            'phase':[0., 1., 4., 0.],\n",
    "            'dX':0.01,\n",
    "            \"noise\": 0.0},\n",
    "#         data_source_type=\"OU\",\n",
    "#         generator_params=dict(\n",
    "#             theta=[1.],\n",
    "#             phi = [1.],\n",
    "#             noise_var = [1.],\n",
    "            \n",
    "#         ),\n",
    "\n",
    "        preprocessor_type=\"WindowedStacker\",\n",
    "        window_length=64,\n",
    "    \n",
    "        agent_type = \"DQN\",\n",
    "        discrete_actions=True,\n",
    "        discrete_action_atoms=3,\n",
    "        double_dqn=True,\n",
    "        nstep_return = 3,\n",
    "        target_update_freq=12000,\n",
    "        rb_size=100_000,\n",
    "        min_rb_size=50_000,\n",
    "        batch_size=32,\n",
    "        discount = 0.999,\n",
    "        lot_unit_value=100_000,\n",
    "    \n",
    "    \n",
    "        model_class=\"ConvModel\",\n",
    "        lr=1e-3,\n",
    "\n",
    "    )\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.70201726, -0.57958206,  1.19119282,  1.22320072,  0.38948872,\n",
       "         1.55826229,  0.7655399 , -0.86206551, -0.12186737,  1.88394256]),\n",
       " array([-1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.,  1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@numba.njit\n",
    "def ternarize_array(arr):\n",
    "    out = np.empty_like(arr)\n",
    "    out[arr<0.] = -1.\n",
    "    out[arr==0.] = 0.\n",
    "    out[arr>0.] = 1.\n",
    "    return out\n",
    "# test\n",
    "ara = np.random.randn(10)\n",
    "out = ternarize_array(ara)\n",
    "ara, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.32922778],\n",
       "        [-0.3712605 ],\n",
       "        [ 0.8276056 ],\n",
       "        [ 1.49226636],\n",
       "        [ 1.33125458],\n",
       "        [ 0.48690322],\n",
       "        [ 2.00076689],\n",
       "        [-0.32110771],\n",
       "        [ 1.32094653],\n",
       "        [-2.17859303]]),\n",
       " array([[-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [-1.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @numba.vectorize([numba.int32(numba.int32),\n",
    "#                   numba.int64(numba.int64),\n",
    "#                   numba.float32(numba.float32),\n",
    "#                   numba.float64(numba.float64)])\n",
    "\n",
    "@numba.vectorize([numba.float32(numba.float32),\n",
    "                  numba.float64(numba.float64)])\n",
    "def ternarize_array(val):\n",
    "    if val < 0:\n",
    "        out = -1.\n",
    "    elif val > 0.:\n",
    "        out = 1.\n",
    "    else:\n",
    "        out = 0.\n",
    "    return out\n",
    "# test\n",
    "ara = np.random.randn(10, 1)\n",
    "out = ternarize_array(ara)\n",
    "ara, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     3,
     13
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BrokerResponse:\n",
    "    event: str\n",
    "    timestamp: int\n",
    "    transPrice: float\n",
    "    transUnits: float\n",
    "    transCost: float\n",
    "    riskInfo: object\n",
    "    marginCall: bool\n",
    "\n",
    "@dataclass\n",
    "class EnvInfo:\n",
    "    brokerResponse: BrokerResponse\n",
    "    exiting: bool\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EnvTernary(EnvC):\n",
    "    def step(self, actions: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        If actions is None, no transaction is attempted and dataSource is iterated to\n",
    "        get new prices\n",
    "        If actions is passed, a transaction/s is attempted. Can only Reverse Positions, not add to \n",
    "        or close.\n",
    "        Hence EnvBinary.\n",
    "        \"\"\"\n",
    "\n",
    "        if actions is None: # no transaction\n",
    "            prevEq = self.portfolio.equity\n",
    "            newPrices = self.dataSource.getData()\n",
    "            newEq = self.portfolio.equity\n",
    "            reward = (newEq-prevEq) / prevEq\n",
    "            risk = self.portfolio.checkRisk()\n",
    "            done = False if risk == RiskInfo.green else True\n",
    "\n",
    "            return (State(newPrices, np.array(self.ledgerNormed, copy=True),\n",
    "                          self.dataSource.currentTime),\n",
    "                    reward, done, EnvInfo(BrokerResponse(\"\", 0, 0., 0., 0., \n",
    "                                                             risk, done), False))\n",
    "        else:\n",
    "            if not isinstance(actions, np.ndarray):\n",
    "                raise TypeError(\"action must be an np array\")\n",
    "\n",
    "            prevEq = self.portfolio.equity\n",
    "            newPrices = self.dataSource.getData()\n",
    "            newEq = self.portfolio.equity\n",
    "            reward = newEq / prevEq \n",
    "            reward = math.log(max(reward, 0.3))\n",
    "\n",
    "            ledger_ternary = self.ledgerTernary\n",
    "            actions_ternary = actions - 1\n",
    "#             actions_ternary[ledger_ternary == actions_ternary] = 0.\n",
    "            units = (0.2*self.availableMargin) / self.currentPrices\n",
    "            transactions = actions_ternary * units\n",
    "\n",
    "#             exiting = False\n",
    "            assets = np.where(transactions!=0.)[0]\n",
    "            for i, asset in enumerate(assets): # implicit if len(assets)\n",
    "                reward -= self.transaction_cost_rel\n",
    "#                 if self.ledger[i] != 0:\n",
    "#                     exiting = True\n",
    "#                     self.broker.close(int(asset))\n",
    "                    \n",
    "            broker_response_multi = self.broker.handleTransaction(transactions)\n",
    "            \n",
    "            done = False\n",
    "            if broker_response_multi.marginCall:\n",
    "                done=True\n",
    "            for _risk in broker_response_multi.riskInfo:\n",
    "                if _risk != RiskInfo.green:\n",
    "                    done = True\n",
    "#             if exiting:\n",
    "#                 done = True\n",
    "            if self.equity < 0.1 * self.portfolio.initCash:\n",
    "                done = True\n",
    "                print('equity: ', self.equity)\n",
    "\n",
    "            return (State(newPrices, np.array(self.ledgerNormed, copy=True),\n",
    "                          self.dataSource.currentTime),\n",
    "                    reward, done, EnvInfo(broker_response_multi, 0.))\n",
    "\n",
    "    @property\n",
    "    def ledgerTernary(self):\n",
    "        ara = np.array(self.ledger, copy=True)\n",
    "        return ternarize_array(ara)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def make_env(config):\n",
    "#     assets = Assets(config.assets)\n",
    "#     env = EnvTernary(config.data_source_type, assets, config.init_cash, config)\n",
    "#     env.lot_unit = config.lot_unit_value\n",
    "#     env.action_atoms = config.discrete_action_atoms\n",
    "#     env.transaction_cost_rel = config.transaction_cost_rel\n",
    "# #     env.setTransactionCost\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "reward: 0.003362993925419609\n",
      "[7.21755705] 1000000.0 1000000.0 [0.] 1000000.0\n"
     ]
    }
   ],
   "source": [
    "env = make_env(config)\n",
    "srdi = env.step(np.array([10000]))\n",
    "print('reward:', srdi[1])\n",
    "print('reward:', env.step()[1])\n",
    "env.step(np.array([0]))\n",
    "env.reset()\n",
    "print(env.currentPrices, env.cash, env.equity, env.ledger, env.availableMargin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([757.38695427])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srdi[3].brokerResponse.transactionCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import sklearn.preprocessing\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, window_len):\n",
    "        self.k = window_len\n",
    "        self.min_tf = self.k\n",
    "        self.price_buffer = deque(maxlen=self.k)\n",
    "        self.portfolio_buffer = deque(maxlen=self.k)\n",
    "        self.time_buffer = deque(maxlen=self.k)\n",
    "        self.feature_output_size = 12\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.price_buffer)\n",
    "\n",
    "    def stream_srdi(self, srdi):\n",
    "        self.price_buffer.append(srdi[0].price)\n",
    "        self.portfolio_buffer.append(srdi[0].portfolio)\n",
    "        self.time_buffer.append(srdi[0].timestamp)\n",
    "\n",
    "    def stream_state(self, state):\n",
    "        self.price_buffer.append(np.array(state.price, copy=True))\n",
    "        self.portfolio_buffer.append(np.array(state.portfolio, copy=True))\n",
    "        self.time_buffer.append(np.array(state.timestamp, copy=True))\n",
    "\n",
    "    def stream(self, data):\n",
    "        if isinstance(data, tuple): # assume srdi\n",
    "            self.stream_srdi(data)\n",
    "        elif isinstance(data, (StateA, State)):\n",
    "            self.stream_state(data)\n",
    "\n",
    "    def current_data(self):\n",
    "        prices = np.array(self.price_buffer, copy=True)\n",
    "        prices = sklearn.preprocessing.minmax_scale(prices)\n",
    "        features = np.empty(self.feature_output_size)\n",
    "        for i, window in enumerate([3, 5, 7, 11]):\n",
    "            features[i] = np.mean(prices[-window:])\n",
    "            features[i+4] = np.var(prices[-window:])\n",
    "        features[8] = prices.min()\n",
    "        features[9] = prices.max()\n",
    "        features[10] = prices[0]\n",
    "        features[11] = prices[-1]\n",
    "        return State(features.reshape(-1, 1) ,\n",
    "                     self.portfolio_buffer[-1],\n",
    "                     self.time_buffer[-1])\n",
    "\n",
    "    def initialize_history(self, env):\n",
    "        while len(self) < self.k:\n",
    "            _state, reward, done, info = env.step()\n",
    "            self.stream_state(_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(config):\n",
    "    if config.preprocessor_type == \"Custom\":\n",
    "        return Preprocessor(config.preprocessor_config.window_length)\n",
    "    return _make_preprocessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<madigan.environments.cpp.build.env.State at 0x7f0d7cf80db0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "figsize(8, 6)\n",
    "\n",
    "preprocessor = make_preprocessor(config)\n",
    "env.currentPrices\n",
    "preprocessor.initialize_history(env)\n",
    "plt.plot(preprocessor.current_data().price)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from madigan.fleet.conv_model import calc_conv_out_shape\n",
    "\n",
    "class DuelingHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Replace normal output layer in DQN with this for Dueling Network Architectures\n",
    "    See: https://arxiv.org/pdf/1511.06581.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.value_net = nn.Linear(d_in, 1)\n",
    "        self.adv_net = nn.Linear(d_in, d_out)\n",
    "    def forward(self, x):\n",
    "        value = self.value_net(x)\n",
    "        adv = self.adv_net(x)\n",
    "        qvals = value + adv - adv.mean(-1, keepdim=True)\n",
    "        return qvals\n",
    "    \n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, config, feature_input_size):\n",
    "        super().__init__()\n",
    "        nassets = len(config.assets)\n",
    "        d_model = config.agent_config.model_config.d_model\n",
    "        nactions = config.discrete_action_atoms\n",
    "        self.conv1 = nn.Conv1d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv1d(32, 32, 5)\n",
    "        out_shape = calc_conv_out_shape(feature_input_size, [self.conv1, self.conv2])\n",
    "        self.fc1 = nn.Linear(nassets+out_shape[0]*32, d_model)\n",
    "        self.output_head = nn.Linear(d_model, nactions)\n",
    "        self.act = nn.GELU()     \n",
    "        \n",
    "    def forward(self, state):\n",
    "        price = state.price.transpose(-1, -2)\n",
    "        portfolio = state.portfolio\n",
    "        price_emb = self.act(self.conv1(price))\n",
    "        price_emb = self.act(self.conv2(price_emb)).view(price.shape[0], -1)\n",
    "\n",
    "        full_emb = torch.cat([price_emb, portfolio], dim=-1)\n",
    "        out = self.act(self.fc1(full_emb))\n",
    "\n",
    "        logits = self.output_head(out)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from madigan.utils import DiscreteRangeSpace, DiscreteActionSpace\n",
    "\n",
    "# class EpisodeBuffer:\n",
    "#     def __init__(self, rollout_len=5000):\n",
    "#         self._buffer = []\n",
    "#         self.rollout_len = rollout_len\n",
    "        \n",
    "#     def add(self, sarsd):\n",
    "#         if len(self._buffer) < self.rollout_len:\n",
    "#             self._buffer.append(sarsd)\n",
    "#         else:\n",
    "#             warnings.warning(\"Max length of experience buffer exceeded\")\n",
    "            \n",
    "#     def get_episode(self):\n",
    "#         return self._buffer\n",
    "\n",
    "class AgentCNN:\n",
    "    def __init__(self, config, feature_input_size, env, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._env = env\n",
    "        \n",
    "        self.action_atoms = config.discrete_action_atoms\n",
    "\n",
    "        self.lot_unit_value = config.lot_unit_value\n",
    "        actions = [self.lot_unit_value*action - self.action_atoms//2 for action in range(self.action_atoms)]\n",
    "        probs = [1/len(actions) for i in actions]\n",
    "        self._action_space = DiscreteRangeSpace((0, 2), len(config.assets))\n",
    "\n",
    "        self.actor = ConvModel(config, feature_input_size).to(device).float()\n",
    "        self.critic_b = ConvModel(config, feature_input_size).to(device).float()\n",
    "        self.critic_t = ConvModel(config, feature_input_size).to(device).float()\n",
    "        \n",
    "        self.dueling = config.agent_config.model_config.dueling\n",
    "        self.double_dqn = config.agent_config.double_dqn\n",
    "        self.discount = config.agent_config.discount\n",
    "        \n",
    "        self.opt_critic = torch.optim.Adam(self.critic_b.parameters(),\n",
    "                                           lr=config.agent_config.optim_config.lr_critic)\n",
    "        self.opt_actor = torch.optim.Adam(self.actor.parameters(), \n",
    "                                           lr=config.agent_config.optim_config.lr_actor)\n",
    "        self.tau_soft_update = config.agent_config.tau_soft_update\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "    @property\n",
    "    def env(self):\n",
    "        return self._env\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._action_space\n",
    "    \n",
    "    def prep_state(self, state, batch=False):\n",
    "        if not batch:\n",
    "            price = torch.as_tensor(state.price[None, ...], dtype=torch.float32).to(self.device)\n",
    "            port = torch.as_tensor(state.portfolio[None, -1], dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            price = torch.as_tensor(state.price, dtype=torch.float32).to(self.device)\n",
    "            port = torch.as_tensor(state.portfolio[:, -1], dtype=torch.float32).to(self.device)\n",
    "#         timestamp = torch.as_tensor(state.timestamp)\n",
    "        return State(price, port, state.timestamp)\n",
    "    \n",
    "    def get_qvals(self, state, target=False):\n",
    "        state = self.prep_state(state)\n",
    "        if target:\n",
    "            qvals = self.critic_t(state)\n",
    "        else:\n",
    "            qvals = self.critic_b(state)\n",
    "        return qvals\n",
    "    \n",
    "    def get_policy(self, state):\n",
    "        state = self.prep_state(state)\n",
    "        logits = self.actor(state)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def action_to_transaction(self, actions):\n",
    "        units = 0.1 * self.env.availableMargin / self.env.currentPrices\n",
    "        actions_ternary = (actions - (self.action_atoms // 2)).cpu().numpy()\n",
    "        return units * actions_ternary\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state):\n",
    "        policy = self.get_policy(state)\n",
    "        actions = policy.sample()\n",
    "        return self.action_to_transaction(actions)\n",
    "        \n",
    "    def prep_sarsd(self, sarsd):\n",
    "        state = self.prep_state(sarsd.state, batch=True)\n",
    "#         action = np.rint(sarsd.action // self.lot_unit_value) + self.action_atoms//2\n",
    "        action = ternarize_array(sarsd.action) + self.action_atoms // 2\n",
    "        action = torch.as_tensor(action, dtype=torch.long, device=self.device)[..., 0]\n",
    "        reward = torch.as_tensor(sarsd.reward, dtype=torch.float32, device=self.device)\n",
    "        next_state = self.prep_state(sarsd.next_state, batch=True)\n",
    "        done = torch.as_tensor(sarsd.done, dtype=torch.bool, device=self.device)\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def loss_fn(self, Qt, Gt):\n",
    "        return F.smooth_l1_loss(Qt, Gt)\n",
    "    \n",
    "    \n",
    "    def train_step(self, sarsd_critic, sarsd_actor):\n",
    "        critic_metrics = self.train_step_critic(sarsd_critic)\n",
    "        actor_metrics = self.train_step_actor(sarsd_actor)\n",
    "        return critic_metrics, actor_metrics\n",
    "    \n",
    "    def train_step_critic(self, sarsd):\n",
    "        state, action, reward, next_state, done = self.prep_sarsd(sarsd)\n",
    "        self.opt_critic.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            next_policy = Categorical(logits=self.actor(next_state))    \n",
    "            next_actions = next_policy.sample()\n",
    "            one_hot = F.one_hot(next_actions, self.action_atoms).to(self.device)\n",
    "            greedy_qvals_next = (self.critic_t(next_state)*one_hot).sum(-1)\n",
    "            Gt = reward + (~done*self.discount * greedy_qvals_next)\n",
    "\n",
    "        action_mask = F.one_hot(action, self.action_atoms).to(self.device)\n",
    "        qvals = self.critic_b(state)\n",
    "        Qt = (qvals*action_mask).sum(-1)\n",
    "        \n",
    "        loss = self.loss_fn(Qt, Gt)\n",
    "        loss.backward()\n",
    "        self.opt_critic.step()\n",
    "        \n",
    "        td_error = (Gt-Qt).abs().mean().detach().item()\n",
    "        \n",
    "        self.update_target()\n",
    "        \n",
    "        return {'loss': loss.detach().item(), 'td_error': td_error, 'Qt': Qt.detach(), 'Gt': Gt.detach()}\n",
    "    \n",
    "    def train_step_actor(self, sarsd_actor):\n",
    "        state, action, reward, next_state, done = self.prep_sarsd(sarsd_actor)\n",
    "        logits = self.actor(state)\n",
    "        logp = Categorical(logits=logits).log_prob(action)\n",
    "        \n",
    "        action_mask = F.one_hot(action, self.action_atoms).to(action.device)\n",
    "        qvals = self.critic_t(state)\n",
    "        Gt = (qvals*action_mask).sum(-1)\n",
    "        assert Gt.shape == logp.shape\n",
    "#         print(Gt.shape, logp.shape)\n",
    "#         print((Gt * logp).shape)\n",
    "        loss = (-logp * Gt).mean()\n",
    "        self.opt_actor.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt_actor.step()\n",
    "        return {'loss': loss.detach().item()}\n",
    "    \n",
    "    def update_target(self):\n",
    "        \"\"\"\n",
    "        Soft Update \n",
    "        \"\"\"\n",
    "        for behaviour, target in zip(self.critic_b.parameters(), self.critic_t.parameters()):\n",
    "            target.data.copy_(self.tau_soft_update * behaviour.data + \\\n",
    "                              (1.-self.tau_soft_update)*target.data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0470, -0.0649, -0.0060]], device='cuda:0', grad_fn=<AddmmBackward>) tensor([0], device='cuda:0') [-4153.13647216]\n"
     ]
    }
   ],
   "source": [
    "agent = AgentCNN(config, preprocessor.feature_output_size, env, device=device)\n",
    "\n",
    "x = preprocessor.current_data()\n",
    "# x = agent.prep_state(x)\n",
    "qvals = agent.get_qvals(x)\n",
    "policy = agent.get_policy(x)\n",
    "action = policy.sample()\n",
    "transaction = agent.action_to_transaction(action)\n",
    "print(qvals, action, transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "def trainer(agent, env, preprocessor, config):\n",
    "    rb = ReplayBuffer(config.rb_size)\n",
    "    episode_length = config.episode_length\n",
    "    \n",
    "    nstep_buffer = []\n",
    "    nstep=config.nstep_return\n",
    "    min_rb_size = config.min_rb_size\n",
    "    target_update_freq = config.target_update_freq\n",
    "    \n",
    "    eps = 1.\n",
    "    eps_decay = config.expl_eps_decay\n",
    "    eps_min = config.expl_eps_min\n",
    "    env.reset()\n",
    "    preprocessor.initialize_history(env)\n",
    "    state = preprocessor.current_data()\n",
    "    i = 0\n",
    "    running_reward = 0.\n",
    "    running_cost = 0.\n",
    "    while True:\n",
    "        critic_metrics=None\n",
    "        actor_metrics=None\n",
    "        \n",
    "        if random.random() < eps:\n",
    "            units = 0.1 * env.availableMargin / env.currentPrices\n",
    "            actions = np.random.choice([-1., 0., 1.], len(units))\n",
    "            actions *= units\n",
    "        else:\n",
    "            actions = agent(state)\n",
    "        eps = max(eps_min, eps*eps_decay)\n",
    "        \n",
    "#         Prevent double positon\n",
    "#         for i, action in enumerate(actions):\n",
    "#             if np.sign(action) == np.sign(env.ledger):\n",
    "#                 actions[i] = 0.\n",
    "                \n",
    "        _next_state, reward, done, info = env.step(actions)\n",
    "        for cost in info.brokerResponse.transactionCost:\n",
    "            running_cost += cost\n",
    "#         reward = max(-1., min(reward, 1.))\n",
    "        preprocessor.stream_state(_next_state)\n",
    "        next_state = preprocessor.current_data()\n",
    "        if done:\n",
    "            reward = -1.\n",
    "        running_reward += reward\n",
    "        sarsd = SARSD(state, actions, reward, next_state, done)\n",
    "        rb.add(sarsd)\n",
    "#         nstep_buffer.append(sarsd)\n",
    "#         if len(nstep_buffer) == nstep:\n",
    "#             _reward = sum([dat.reward for dat in nstep_buffer])\n",
    "#             nstep_sarsd = nstep_buffer.pop(0)\n",
    "#             nstep_sarsd.reward = _reward\n",
    "#             rb.add(nstep_sarsd)\n",
    "#             running_reward += _reward\n",
    "        \n",
    "        if done:\n",
    "            env.reset()\n",
    "            preprocessor.initialize_history(env)\n",
    "            state = preprocessor.current_data()\n",
    "            print('running_reward: ', running_reward, 'running_cost: ', running_cost)\n",
    "\n",
    "            running_reward = 0.\n",
    "            running_cost = 0.\n",
    "        else:\n",
    "            state = next_state\n",
    "        if len(rb) >= min_rb_size:\n",
    "            sarsd_critic = rb.sample(config.batch_size)\n",
    "            critic_metrics = agent.train_step_critic(sarsd_critic)\n",
    "            critic_metrics['eps'] = eps\n",
    "            critic_metrics['running_reward'] = running_reward\n",
    "\n",
    "            if i % episode_length == 0:\n",
    "#                 try:\n",
    "                sarsd_actor = rb.get_latest(episode_length)\n",
    "#                 except:\n",
    "#                     import ipdb; ipdb.set_trace()\n",
    "                actor_metrics = agent.train_step_actor(sarsd_actor)\n",
    "            \n",
    "        yield {'critic': critic_metrics, 'actor': actor_metrics}\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basepath': '/media/hemu/Data/Markets/farm',\n",
       " 'experiment_id': 'SineQ',\n",
       " 'parent_id': '',\n",
       " 'overwrite_exp': False,\n",
       " 'transaction_cost_abs': 0.0,\n",
       " 'transaction_cost_rel': 0.02,\n",
       " 'slippage_abs': 0.0,\n",
       " 'slippage_rel': 0.0,\n",
       " 'env_type': 'Synth',\n",
       " 'data_source_type': 'SineAdder',\n",
       " 'init_cash': 1000000,\n",
       " 'required_margin': 1.0,\n",
       " 'maintenance_margin': 0.25,\n",
       " 'generator_params': {'freq': [2.2, 4.1, 1.0, 3.0],\n",
       "  'mu': [0.6, 0.3, 2.0, 4.2],\n",
       "  'amp': [0.5, 0.2, 0.4, 1.2],\n",
       "  'phase': [0.0, 1.0, 4.0, 0.0],\n",
       "  'dX': 0.01,\n",
       "  'noise': 0.0},\n",
       " 'assets': ['sine1'],\n",
       " 'lot_unit_value': 10000,\n",
       " 'n_assets': 1,\n",
       " 'discrete_actions': True,\n",
       " 'discrete_action_atoms': 3,\n",
       " 'preprocessor_type': 'WindowedStacker',\n",
       " 'preprocessor_config': {'window_length': 64},\n",
       " 'agent_type': 'DQN',\n",
       " 'agent_config': {'type': 'DQN',\n",
       "  'basepath': '/media/hemu/Data/Markets/farm',\n",
       "  'discrete_action_atoms': 3,\n",
       "  'model_config': {'model_class': 'ConvModel',\n",
       "   'd_model': 256,\n",
       "   'n_layers': 4,\n",
       "   'n_feats': 1,\n",
       "   'action_atoms': 3,\n",
       "   'n_assets': 1,\n",
       "   'min_tf': 64,\n",
       "   'dueling': True,\n",
       "   'iqn': True,\n",
       "   'nTau1': 32,\n",
       "   'nTau2': 8,\n",
       "   'tau_embed_size': 64,\n",
       "   'discrete_actions': True,\n",
       "   'discrete_action_atoms': 3,\n",
       "   'lot_unit_value': 10000},\n",
       "  'optim_config': {'type': 'Adam',\n",
       "   'lr': 0.001,\n",
       "   'lr_critic': 0.001,\n",
       "   'lr_actor': 0.0001,\n",
       "   'eps': 1e-08,\n",
       "   'momentum': 0.9,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'weight_decay': 0},\n",
       "  'double_dqn': True,\n",
       "  'dueling': True,\n",
       "  'iqn': True,\n",
       "  'nTau1': 32,\n",
       "  'nTau2': 8,\n",
       "  'k_huber': 1,\n",
       "  'tau_embed_size': 64,\n",
       "  'discount': 0.999,\n",
       "  'nstep_return': 5,\n",
       "  'action_atoms': 3,\n",
       "  'tau_soft_update': 0.0001,\n",
       "  'greedy_eps_testing': 0.0},\n",
       " 'model_config': {'model_class': 'ConvModel',\n",
       "  'd_model': 256,\n",
       "  'n_layers': 4,\n",
       "  'n_feats': 1,\n",
       "  'action_atoms': 3,\n",
       "  'n_assets': 1,\n",
       "  'min_tf': 64,\n",
       "  'dueling': True,\n",
       "  'iqn': True,\n",
       "  'nTau1': 32,\n",
       "  'nTau2': 8,\n",
       "  'tau_embed_size': 64,\n",
       "  'discrete_actions': True,\n",
       "  'discrete_action_atoms': 3,\n",
       "  'lot_unit_value': 10000},\n",
       " 'optim_config': {'type': 'Adam',\n",
       "  'lr': 0.001,\n",
       "  'lr_critic': 0.001,\n",
       "  'lr_actor': 0.0001,\n",
       "  'eps': 1e-08,\n",
       "  'momentum': 0.9,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'weight_decay': 0},\n",
       " 'nsteps': 1000000,\n",
       " 'test_steps': 1000,\n",
       " 'rb_size': 100000,\n",
       " 'episode_length': 1024,\n",
       " 'min_rb_size': 50000,\n",
       " 'train_freq': 4,\n",
       " 'target_update_freq': 32000,\n",
       " 'test_freq': 32000,\n",
       " 'log_freq': 10000,\n",
       " 'model_save_freq': 64000,\n",
       " 'min_tf': 64,\n",
       " 'batch_size': 34,\n",
       " 'nstep_return': 5,\n",
       " 'expl_eps': 1.0,\n",
       " 'expl_eps_min': 0.1,\n",
       " 'expl_eps_decay': 0.999999,\n",
       " 'reward_clip': (-1.0, 1.0)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = make_config(\n",
    "        experiment_id=\"SineQ\",\n",
    "        basepath=\"/media/hemu/Data/Markets/farm\",\n",
    "        overwrite_exp=False,\n",
    "        transaction_cost_rel=0.02,\n",
    "    \n",
    "    \n",
    "        test_steps=1_000,\n",
    "        nsteps=1_000_000,\n",
    "    \n",
    "        assets=[\"sine1\"],\n",
    "#         data_source_type=\"Triangle\",\n",
    "#         generator_params={\n",
    "#         'freq':[2.],\n",
    "#         'mu':[0.6],\n",
    "#         'amp':[.5],\n",
    "#         'phase':[0.],\n",
    "#         'dX':0.1,\n",
    "#         \"noise\": 0.0},\n",
    "        data_source_type=\"SineAdder\",\n",
    "        generator_params={\n",
    "            'freq':[2.2, 4.1, 1., 3.],\n",
    "            'mu':[.6, 0.3, 2., 4.2],\n",
    "            'amp':[.5, 0.2, 0.4, 1.2],\n",
    "            'phase':[0., 1., 4., 0.],\n",
    "            'dX':0.01,\n",
    "            \"noise\": 0.0},\n",
    "#         data_source_type=\"OU\",\n",
    "#         generator_params=dict(\n",
    "#             theta=[.05],\n",
    "#             phi = [1.],\n",
    "#             noise_var = [1.]\n",
    "#         ),\n",
    "        preprocessor_type=\"WindowedStacker\",\n",
    "        window_length=64,\n",
    "    \n",
    "        agent_type = \"DQN\",\n",
    "        discrete_actions=True,\n",
    "        discrete_action_atoms=3,\n",
    "        double_dqn=True,\n",
    "        dueling=True,\n",
    "        iqn=True,\n",
    "        nTau1=32,\n",
    "        nTau2=8,\n",
    "        k_huber=1,\n",
    "        nstep_return = 5,\n",
    "        tau_soft_update=1e-4,\n",
    "        rb_size=100_000,\n",
    "        min_rb_size=50_000,\n",
    "        batch_size=34,\n",
    "        discount = 0.999,\n",
    "        lot_unit_value=10_000,\n",
    "    \n",
    "        expl_eps_decay=0.999999,\n",
    "    \n",
    "        model_class=\"ConvModel\",\n",
    "        d_model = 256,\n",
    "        lr=1e-3,\n",
    "\n",
    "    )\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(config)\n",
    "\n",
    "# agent = make_agent(config)\n",
    "\n",
    "preprocessor = make_preprocessor(config)\n",
    "preprocessor.initialize_history(env)\n",
    "\n",
    "agent = AgentCNN(config, preprocessor.feature_output_size, env, device=device)\n",
    "# agent = AgentCNN_IQN(config, preprocessor.feature_output_size, env, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing replay buffer with min # of experiences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7ee6ab0e26485196149c1bdbc225b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_reward:  -1.0656007718017766 running_cost:  460381.14474455564\n",
      "running_reward:  -1.102025037948482 running_cost:  75792.4302342917\n",
      "running_reward:  -1.188983157308254 running_cost:  122956.49069781054\n",
      "running_reward:  -1.5764482143686485 running_cost:  624209.8612508873\n",
      "running_reward:  -1.154149827553709 running_cost:  93153.29358508077\n",
      "running_reward:  -1.1851633510396598 running_cost:  156855.88337114532\n",
      "running_reward:  -1.1208715312465622 running_cost:  108079.16356181071\n",
      "running_reward:  -1.0775826594365214 running_cost:  153532.19819459083\n",
      "running_reward:  -1.093536496073257 running_cost:  37961.720001132475\n",
      "running_reward:  -1.1247693543932231 running_cost:  68002.97922264614\n",
      "running_reward:  -2.6668211448378183 running_cost:  252987.18786966606\n",
      "running_reward:  -1.1426145118469169 running_cost:  120363.7950280571\n",
      "running_reward:  -0.960861747454846 running_cost:  155413.8248006689\n",
      "running_reward:  -1.035451674911397 running_cost:  148681.58518633785\n",
      "running_reward:  -1.189247320690775 running_cost:  108995.69186907454\n",
      "running_reward:  -1.033041891459655 running_cost:  242771.61366469198\n",
      "running_reward:  -1.9829365967443064 running_cost:  791496.8349740567\n",
      "running_reward:  -1.0812853154029733 running_cost:  94004.21835179393\n",
      "running_reward:  -1.0276531815354717 running_cost:  132219.6574657217\n",
      "running_reward:  -0.9876974664432635 running_cost:  113409.58387339549\n",
      "running_reward:  -1.042852197283352 running_cost:  159674.55778568654\n",
      "running_reward:  -0.9990938090284777 running_cost:  220241.68549529702\n",
      "running_reward:  -1.05469112350726 running_cost:  90525.40872065477\n",
      "running_reward:  -0.8428887728672783 running_cost:  227487.4625904744\n",
      "running_reward:  -1.0897732391523225 running_cost:  34250.53556111361\n",
      "running_reward:  -1.0810311257118572 running_cost:  65894.70977320385\n",
      "running_reward:  -1.0063946098440857 running_cost:  208009.1536630426\n",
      "running_reward:  -0.8977773727836487 running_cost:  471853.08189447556\n",
      "running_reward:  -1.157009596694962 running_cost:  57797.88638321597\n",
      "running_reward:  -1.3229235244425093 running_cost:  150040.5207901604\n",
      "running_reward:  -1.1141155828095108 running_cost:  268837.72756341123\n",
      "running_reward:  -1.221567015562007 running_cost:  325938.4417206908\n",
      "running_reward:  -2.4877648555690364 running_cost:  332599.00540576654\n",
      "running_reward:  -1.0400938729706652 running_cost:  91062.02159915492\n",
      "running_reward:  -0.7874349504740603 running_cost:  741983.42656653\n",
      "running_reward:  -0.9994642422312281 running_cost:  78277.0561357532\n",
      "running_reward:  -2.47827914590203 running_cost:  443811.29271845863\n",
      "running_reward:  -1.0903889047997883 running_cost:  213261.62221831607\n",
      "running_reward:  -2.459141048741545 running_cost:  423535.3497784337\n",
      "running_reward:  -2.2548934563959238 running_cost:  645024.7098805666\n",
      "running_reward:  -1.164145578997472 running_cost:  58960.53817659992\n",
      "running_reward:  -1.1288265433411622 running_cost:  105082.51833075525\n",
      "running_reward:  -1.100428492197877 running_cost:  64001.336901729424\n",
      "running_reward:  -0.9409538146454639 running_cost:  488162.9077030955\n",
      "running_reward:  -1.8356940362112497 running_cost:  696887.91968999\n",
      "running_reward:  -0.9607201395880653 running_cost:  134441.91086332093\n",
      "running_reward:  -2.399800608936046 running_cost:  321806.65045814845\n",
      "running_reward:  -2.342920434532634 running_cost:  565602.542581753\n",
      "running_reward:  -1.099576929340401 running_cost:  156836.11841885996\n",
      "running_reward:  -2.188044450094076 running_cost:  328951.6786970791\n",
      "running_reward:  -1.272427649939194 running_cost:  376752.017664806\n",
      "running_reward:  -0.5339800393587482 running_cost:  893133.0770886994\n",
      "running_reward:  -1.1109110468746974 running_cost:  305409.96927776653\n",
      "running_reward:  -0.9634135763181045 running_cost:  270907.87202907354\n",
      "running_reward:  -0.9091144448191985 running_cost:  365945.5432498811\n",
      "running_reward:  -1.045555434903841 running_cost:  553942.6915859722\n",
      "running_reward:  -1.1199287152031079 running_cost:  31927.82683252086\n",
      "running_reward:  -1.1103400025052483 running_cost:  201126.44008860854\n",
      "running_reward:  -1.9643356751065113 running_cost:  773525.0965392299\n",
      "running_reward:  -1.0097002873350527 running_cost:  100362.85865546562\n",
      "running_reward:  -1.1591870464669294 running_cost:  62117.47677591382\n",
      "running_reward:  -2.1375369318443633 running_cost:  568359.2576146836\n",
      "running_reward:  -1.2228894685758385 running_cost:  92150.30850315606\n",
      "running_reward:  -1.104898230663066 running_cost:  254729.37532046743\n",
      "running_reward:  -2.4075581707667064 running_cost:  293463.16469175584\n",
      "running_reward:  -0.9708987544448141 running_cost:  388129.9813637542\n",
      "running_reward:  -0.9908221720876176 running_cost:  297734.86430492747\n",
      "running_reward:  -1.016136954027207 running_cost:  95526.0852659253\n",
      "running_reward:  -1.7682725794461231 running_cost:  751967.1570567494\n",
      "running_reward:  -1.1445441630041981 running_cost:  123487.4250444959\n",
      "running_reward:  -0.8732301975082677 running_cost:  147825.19650469514\n",
      "running_reward:  -1.064032862672408 running_cost:  133486.74831160894\n",
      "running_reward:  -1.2269728146902397 running_cost:  59181.53355279301\n",
      "running_reward:  -1.2644510753447675 running_cost:  763882.3283836407\n",
      "running_reward:  -1.1642949062253756 running_cost:  105087.61430055017\n",
      "running_reward:  -1.0471745759982038 running_cost:  179490.90991135009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop = iter(trainer(agent, env, preprocessor, config))\n",
    "# INITIALIZE REPLAY BUFFER\n",
    "print(\"Initializing replay buffer with min # of experiences\")\n",
    "for i in tqdm(range(config.min_rb_size)):\n",
    "    metrics = next(train_loop)\n",
    "    \n",
    "train_metrics=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9cc635e4f64e0cb356675431f83933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_reward:  -2.4537622281614135 running_cost:  383545.1321035775\n",
      "running_reward:  -1.1456058576952977 running_cost:  57185.64053170606\n",
      "running_reward:  -2.3323015821250275 running_cost:  408731.459224541\n",
      "running_reward:  -1.0732934546331259 running_cost:  44613.934542242496\n",
      "running_reward:  -1.176087510126417 running_cost:  608726.9333533916\n",
      "running_reward:  -2.630100096735818 running_cost:  333780.0900976375\n",
      "running_reward:  -2.534467749441628 running_cost:  442898.39516490226\n",
      "running_reward:  -1.0647524689923564 running_cost:  65465.507919193806\n",
      "running_reward:  -1.0579303157321243 running_cost:  150105.73463668264\n",
      "running_reward:  -1.1326293847689313 running_cost:  245320.97970600502\n",
      "running_reward:  -2.263127637433036 running_cost:  516743.5288669549\n",
      "running_reward:  -2.2154533064097293 running_cost:  378577.27129161556\n",
      "running_reward:  -1.9017282842893326 running_cost:  725390.7287955959\n",
      "running_reward:  -1.1910576744753423 running_cost:  76884.9744790465\n",
      "running_reward:  -1.0551483968383 running_cost:  35497.8102565683\n",
      "running_reward:  -1.1154344036185906 running_cost:  31038.861086684294\n",
      "running_reward:  -1.3221536501818867 running_cost:  323901.7279083116\n",
      "running_reward:  -1.1109342810211837 running_cost:  28898.50898950748\n",
      "running_reward:  -1.0928470620414827 running_cost:  115075.9345895011\n",
      "running_reward:  -0.9999977452132139 running_cost:  517336.105580935\n",
      "running_reward:  -1.4843554975369746 running_cost:  508855.82143073814\n",
      "running_reward:  -1.3272170735060664 running_cost:  289569.61035866337\n",
      "running_reward:  -1.1859961230705394 running_cost:  148509.1407034574\n",
      "running_reward:  -0.9112040286220764 running_cost:  830350.2378557548\n",
      "running_reward:  -2.4022880781195624 running_cost:  450850.32053306914\n",
      "running_reward:  -0.9784926675698966 running_cost:  186769.94238040154\n",
      "running_reward:  -1.0022387350277329 running_cost:  360624.76670935\n",
      "running_reward:  -1.1987313277436034 running_cost:  48896.52641064321\n",
      "running_reward:  -1.0754302373325753 running_cost:  130041.00960527027\n",
      "running_reward:  -1.1021480080666364 running_cost:  182028.4645375325\n",
      "running_reward:  -1.057957375659778 running_cost:  134264.47533704832\n",
      "running_reward:  -1.6461180661065993 running_cost:  754185.4234868586\n",
      "running_reward:  -2.5518143583624044 running_cost:  274344.55762228207\n",
      "running_reward:  -1.0582321021244954 running_cost:  242494.96645182616\n",
      "running_reward:  -1.023334918868077 running_cost:  104553.67887580258\n",
      "running_reward:  -1.0458222540170297 running_cost:  121261.5266623066\n",
      "running_reward:  -1.3156391718364053 running_cost:  371271.88764491415\n",
      "running_reward:  -2.3173919108832917 running_cost:  499823.62264172576\n",
      "running_reward:  -0.9655710808104255 running_cost:  347836.0833066276\n",
      "running_reward:  -2.329632107811542 running_cost:  506468.4502178761\n",
      "running_reward:  -1.0969388006873504 running_cost:  249956.60702488618\n",
      "running_reward:  -2.5137882866807555 running_cost:  323586.0821155765\n",
      "running_reward:  -1.7878928287427875 running_cost:  775154.8948508011\n",
      "running_reward:  -1.042677941593634 running_cost:  49970.53474366039\n",
      "running_reward:  -1.1247697310149858 running_cost:  128259.54439709242\n",
      "running_reward:  -0.9989971895316764 running_cost:  251688.06322623807\n",
      "running_reward:  -1.0512709840988896 running_cost:  175423.44975125286\n",
      "running_reward:  -0.9588774578416746 running_cost:  237344.49887980445\n",
      "running_reward:  -0.9660411167817162 running_cost:  187070.6166834732\n",
      "running_reward:  -2.423514822560489 running_cost:  504362.9465978134\n",
      "running_reward:  -1.1409673016424788 running_cost:  69947.94100397015\n",
      "running_reward:  -1.0274734774740157 running_cost:  111525.87096558615\n",
      "running_reward:  -2.084690177830672 running_cost:  568155.9594963607\n",
      "running_reward:  -1.0504406870195804 running_cost:  535760.1208206889\n",
      "running_reward:  -1.9853942850339985 running_cost:  685489.3460369797\n",
      "running_reward:  -2.1438136496961437 running_cost:  552252.004839708\n",
      "running_reward:  -0.9523486820341026 running_cost:  442972.3261125694\n",
      "running_reward:  -1.047017435887703 running_cost:  105868.87695716436\n",
      "running_reward:  -1.324472889600861 running_cost:  898218.1579672574\n",
      "running_reward:  -1.1569851469649066 running_cost:  168068.6421474992\n",
      "running_reward:  -1.0939880383722091 running_cost:  254189.8926324789\n",
      "running_reward:  -1.1605551026806589 running_cost:  96074.41692901589\n",
      "running_reward:  -1.2320201064644614 running_cost:  73300.76848549707\n",
      "running_reward:  -1.1619649718151832 running_cost:  27459.764783656774\n",
      "running_reward:  -1.1343885643687148 running_cost:  26796.621040701975\n",
      "running_reward:  -1.106226823777358 running_cost:  33129.63609251751\n",
      "running_reward:  -1.1020102121733455 running_cost:  147276.0370890755\n",
      "running_reward:  -0.9559385999494244 running_cost:  203073.3705010853\n",
      "running_reward:  -2.1491931153558315 running_cost:  556080.3882058363\n",
      "running_reward:  -1.0814042001460153 running_cost:  66728.01705773406\n",
      "running_reward:  -1.1090955342522606 running_cost:  57290.02948400378\n",
      "running_reward:  -1.1998630281396747 running_cost:  173253.5541522105\n",
      "running_reward:  -1.160913136891949 running_cost:  114724.10960023364\n",
      "running_reward:  -1.1636031709828882 running_cost:  66861.66257462249\n",
      "running_reward:  -1.1090596358833618 running_cost:  174372.5165715856\n",
      "running_reward:  -1.0748834895512407 running_cost:  141507.71811676538\n",
      "running_reward:  -1.1257155080991785 running_cost:  30917.112150345016\n",
      "running_reward:  -1.4323674621619793 running_cost:  963664.4296302587\n",
      "running_reward:  -1.099124166798889 running_cost:  129487.08624046203\n",
      "running_reward:  -1.0839757307662001 running_cost:  451986.88868973224\n",
      "running_reward:  -0.9846168866941343 running_cost:  316128.46644711116\n",
      "running_reward:  -1.0619159177336583 running_cost:  36933.61800932548\n",
      "running_reward:  -2.4845773644496525 running_cost:  281082.4019063574\n",
      "running_reward:  -2.2245296502773257 running_cost:  612784.8219020099\n",
      "running_reward:  -1.2090389704918838 running_cost:  82710.13226327629\n",
      "running_reward:  -1.2479893721122606 running_cost:  387163.22284196794\n",
      "running_reward:  -2.407887960813011 running_cost:  609979.8781040406\n",
      "running_reward:  -0.8189058899456166 running_cost:  531705.6656544242\n",
      "running_reward:  -2.398818528215189 running_cost:  221887.94911947576\n",
      "running_reward:  -1.1586479399688663 running_cost:  214269.90481316997\n",
      "running_reward:  -1.255780915860674 running_cost:  181093.86146991682\n",
      "running_reward:  -1.122192647895993 running_cost:  285237.4893417383\n",
      "running_reward:  -0.8855895137611669 running_cost:  405868.50007757073\n",
      "running_reward:  -1.3332086061930917 running_cost:  124995.22551976946\n",
      "running_reward:  -0.9504641722510241 running_cost:  355070.00030499\n",
      "running_reward:  -1.9207320633710472 running_cost:  644723.6977470493\n",
      "running_reward:  -0.9892206215890078 running_cost:  120976.32119418563\n",
      "running_reward:  -0.9256738846491194 running_cost:  408276.4236087322\n",
      "running_reward:  -1.1308605136408991 running_cost:  377148.37376136926\n",
      "running_reward:  -2.2763666846060966 running_cost:  650011.7642932828\n",
      "running_reward:  -1.1693642977835 running_cost:  115105.02296807294\n",
      "running_reward:  -1.1039215213296751 running_cost:  67163.98681324015\n",
      "running_reward:  -1.0551442761899847 running_cost:  62791.30147580524\n",
      "running_reward:  -1.023254547140445 running_cost:  449038.17259670305\n",
      "running_reward:  -1.0257847606412924 running_cost:  46540.139466387096\n",
      "running_reward:  -2.5645625451604195 running_cost:  302229.174243714\n",
      "running_reward:  -1.09603859555329 running_cost:  46626.88656365473\n",
      "running_reward:  -1.8447688716874686 running_cost:  808989.778648966\n",
      "running_reward:  -1.8089222167047665 running_cost:  835553.5017837841\n",
      "running_reward:  -1.0838958902784608 running_cost:  98536.249859086\n",
      "running_reward:  -1.1903802145853 running_cost:  107434.60472091837\n",
      "running_reward:  -1.9215037641588548 running_cost:  657955.2862881855\n",
      "running_reward:  -2.2275749127870355 running_cost:  628922.2261751284\n",
      "running_reward:  -1.0701617191806845 running_cost:  350348.28038462735\n",
      "running_reward:  -1.647202303658644 running_cost:  844246.6561469528\n",
      "running_reward:  -1.0363583720651057 running_cost:  87472.78106760951\n",
      "running_reward:  -1.314928157752947 running_cost:  148785.91375586038\n",
      "running_reward:  -1.0012698564179388 running_cost:  214790.60441831718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_reward:  -1.0949075835556632 running_cost:  187970.85426664283\n",
      "running_reward:  -1.2434327083918106 running_cost:  453516.83043384214\n",
      "running_reward:  -1.2123490064942601 running_cost:  224694.8373442148\n",
      "running_reward:  -2.286537201701851 running_cost:  484784.48336531507\n",
      "running_reward:  -1.1058057783933388 running_cost:  36617.000297943996\n",
      "running_reward:  -1.2456805664658728 running_cost:  34220.30701850818\n",
      "running_reward:  -1.6075586945180307 running_cost:  830154.4610860887\n",
      "running_reward:  -1.0159740806061646 running_cost:  221716.06137217223\n",
      "running_reward:  -1.134061935497142 running_cost:  41725.658577548216\n",
      "running_reward:  -2.218877278455569 running_cost:  595241.3283157537\n",
      "running_reward:  -1.1727031539262673 running_cost:  27375.345787571263\n",
      "running_reward:  -1.1631470126559313 running_cost:  80813.01206511163\n",
      "running_reward:  -1.7914146239425532 running_cost:  804939.3057989584\n",
      "running_reward:  -1.1775093430839432 running_cost:  242957.5672269064\n",
      "running_reward:  -1.101132765935195 running_cost:  40724.74176122315\n",
      "running_reward:  -2.3033374661916852 running_cost:  484656.8387014019\n",
      "running_reward:  -2.175675447207613 running_cost:  658953.1820806394\n",
      "running_reward:  -2.520542904009548 running_cost:  360743.28937465977\n",
      "running_reward:  -1.0608773111829146 running_cost:  388338.98937772185\n",
      "running_reward:  -2.1316018597268576 running_cost:  391448.4588951387\n",
      "running_reward:  -0.9999882722979653 running_cost:  168618.52792561636\n",
      "running_reward:  -1.1247585757350462 running_cost:  383408.1876845116\n",
      "running_reward:  -1.1457836877713259 running_cost:  24549.79988520458\n",
      "running_reward:  -2.23979229256666 running_cost:  672742.5099095717\n",
      "running_reward:  -1.023199866165053 running_cost:  46483.15180382658\n",
      "running_reward:  -0.9598030726103929 running_cost:  780144.7993803485\n",
      "running_reward:  -2.371669479587144 running_cost:  430486.9705352825\n",
      "running_reward:  -1.0388204401618741 running_cost:  83563.37654201734\n",
      "running_reward:  -2.4783422911401507 running_cost:  248425.46544579827\n",
      "running_reward:  -0.9865345558692344 running_cost:  485055.8232673768\n",
      "running_reward:  -1.0493497727679761 running_cost:  30226.153961588432\n",
      "running_reward:  -1.1231352431714738 running_cost:  33077.83556751489\n",
      "running_reward:  -1.0869334508887574 running_cost:  64934.456530645766\n",
      "running_reward:  -1.0537178162421785 running_cost:  59859.19721257815\n",
      "running_reward:  -0.9598928970621007 running_cost:  107462.2877078709\n",
      "running_reward:  -1.0915267043672316 running_cost:  30684.651657301958\n",
      "running_reward:  -1.151516032376315 running_cost:  611362.4549462361\n",
      "running_reward:  -1.0224737577289675 running_cost:  454162.6894160105\n",
      "running_reward:  -1.159015632602248 running_cost:  323817.1531009763\n",
      "running_reward:  -1.0710895717767461 running_cost:  62550.77549298311\n",
      "running_reward:  -1.0347198597473046 running_cost:  87038.78883107012\n",
      "running_reward:  -0.9304294943805757 running_cost:  279136.7239873259\n",
      "running_reward:  -1.2050392120380007 running_cost:  474140.17329704174\n",
      "running_reward:  -2.616448869750468 running_cost:  347023.0882077008\n",
      "running_reward:  -1.22751336500035 running_cost:  107497.5694192405\n",
      "running_reward:  -1.0405470766671185 running_cost:  166241.72747307003\n",
      "running_reward:  -1.004640837274318 running_cost:  61034.30295579403\n",
      "running_reward:  -1.193092723642479 running_cost:  241393.20975733732\n",
      "running_reward:  -2.4892142454502433 running_cost:  202326.01453252282\n",
      "running_reward:  -1.133500547883101 running_cost:  347972.2825707679\n",
      "running_reward:  -1.2009921430104296 running_cost:  124596.02582557063\n",
      "running_reward:  -2.1858925042806447 running_cost:  592502.8984334666\n",
      "running_reward:  -1.1033171156760493 running_cost:  175026.59820841928\n",
      "running_reward:  -2.3251273419555094 running_cost:  376005.32767736254\n",
      "running_reward:  -1.166037328007025 running_cost:  29273.85134158115\n",
      "running_reward:  -1.0737156488338215 running_cost:  91942.16547450768\n",
      "running_reward:  -2.141633273901876 running_cost:  830218.4881585732\n",
      "running_reward:  -1.140720462089866 running_cost:  84458.17921608759\n",
      "running_reward:  -1.1597795237337296 running_cost:  1066738.150376814\n",
      "running_reward:  -0.9772179910637402 running_cost:  478229.15909690625\n",
      "running_reward:  -1.202875417374044 running_cost:  107578.37974062354\n",
      "running_reward:  -1.1701966064373421 running_cost:  74702.66633913336\n",
      "running_reward:  -2.50815561817024 running_cost:  211504.23640166328\n",
      "running_reward:  -0.9931929208130315 running_cost:  118455.84748995854\n",
      "running_reward:  -1.17950096450859 running_cost:  76784.78030912587\n",
      "running_reward:  -0.938282504999247 running_cost:  94289.45990298463\n",
      "running_reward:  -2.0318350799729323 running_cost:  669413.0833094135\n",
      "running_reward:  -1.069406563196618 running_cost:  280105.19519115315\n",
      "running_reward:  -1.0472763206626103 running_cost:  672038.673047809\n",
      "running_reward:  -1.1618987726927064 running_cost:  89050.70826443567\n",
      "running_reward:  -1.094326389316543 running_cost:  49523.477685879516\n",
      "running_reward:  -1.094147523651906 running_cost:  99191.80068477435\n",
      "running_reward:  -1.07485798685022 running_cost:  109777.9403071358\n",
      "running_reward:  -1.1218247132997237 running_cost:  29745.625937477602\n",
      "running_reward:  -1.120475542389745 running_cost:  90442.24928829982\n",
      "running_reward:  -1.075091906490177 running_cost:  181418.87275276822\n",
      "running_reward:  -1.1682889770588045 running_cost:  290493.1264179345\n",
      "running_reward:  -0.5787164945961417 running_cost:  574310.8015116702\n",
      "running_reward:  -0.9901751919907513 running_cost:  78906.453403706\n",
      "running_reward:  -0.9778319470676947 running_cost:  159132.78517578222\n",
      "running_reward:  -2.5047492566986302 running_cost:  182876.95106327112\n",
      "running_reward:  -1.1734095037940142 running_cost:  107376.78674946477\n",
      "running_reward:  -2.3609170364957808 running_cost:  452975.07840919757\n",
      "running_reward:  -0.8925152424300018 running_cost:  184972.92927048373\n",
      "running_reward:  -1.1124111792727807 running_cost:  50665.03242076169\n",
      "running_reward:  -1.9681351036793857 running_cost:  861429.0932325703\n",
      "running_reward:  -1.1292389777958274 running_cost:  136188.43155899647\n",
      "running_reward:  -1.1277821637439818 running_cost:  159642.0371450576\n",
      "running_reward:  -1.1492839243433226 running_cost:  163713.2483434895\n",
      "running_reward:  -1.8507840639108322 running_cost:  771559.3643369908\n",
      "running_reward:  -2.490090085993268 running_cost:  404434.4029191549\n",
      "running_reward:  -1.079423403129019 running_cost:  98158.244548615\n",
      "running_reward:  -0.9811151352298335 running_cost:  173057.58720598262\n",
      "running_reward:  -1.0971175734503238 running_cost:  48675.339278953725\n",
      "running_reward:  -1.1192305786752508 running_cost:  55957.28421091357\n",
      "running_reward:  -1.0591190803717363 running_cost:  47148.56563264346\n",
      "running_reward:  -2.1632212176912335 running_cost:  648062.3139982619\n",
      "running_reward:  -1.0396591309270018 running_cost:  77625.6993713974\n",
      "running_reward:  -1.1445836258432822 running_cost:  58529.923662438145\n",
      "running_reward:  -1.154274418398616 running_cost:  57174.63646454147\n",
      "running_reward:  -2.5495426352623167 running_cost:  268507.703375061\n",
      "running_reward:  -2.140712630009058 running_cost:  561427.558037966\n",
      "running_reward:  -1.0003770408278547 running_cost:  71998.05747300861\n",
      "running_reward:  -1.0792293573018945 running_cost:  160573.65100231834\n",
      "running_reward:  -1.0974009896176071 running_cost:  282869.53690538096\n",
      "running_reward:  -1.1837711151857042 running_cost:  139825.89671920167\n",
      "running_reward:  -1.1217861037439207 running_cost:  92532.42990636802\n",
      "running_reward:  -2.1248993501622357 running_cost:  548911.1388419741\n",
      "running_reward:  -2.383878478962888 running_cost:  434984.11296642426\n",
      "running_reward:  -1.0643566475635569 running_cost:  45818.76313267382\n",
      "running_reward:  -1.0480680202083263 running_cost:  116758.07932825998\n",
      "running_reward:  -1.193515761911492 running_cost:  55550.342147583535\n",
      "running_reward:  -2.24337986162906 running_cost:  268421.85357246984\n",
      "running_reward:  -1.1342825862717931 running_cost:  221003.26271190675\n",
      "running_reward:  -2.13857506812802 running_cost:  659317.2919752676\n",
      "running_reward:  -1.0339443851649541 running_cost:  91686.42277124658\n",
      "running_reward:  -2.08725630367906 running_cost:  628923.5174657735\n",
      "running_reward:  -1.215077818008577 running_cost:  99376.92168416298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running_reward:  -1.3483934668571032 running_cost:  342933.64511011343\n",
      "running_reward:  -1.2579054811071941 running_cost:  150487.42396258685\n",
      "running_reward:  -1.0914928771864032 running_cost:  46319.394351492585\n",
      "running_reward:  -1.2661059230472398 running_cost:  538932.6396085549\n",
      "running_reward:  -2.393776379069242 running_cost:  502802.5804648629\n",
      "running_reward:  -1.1530082119874656 running_cost:  128964.39957233294\n",
      "running_reward:  -1.2487392681255312 running_cost:  54729.58329016043\n",
      "running_reward:  -2.510316250623383 running_cost:  451710.3482258693\n",
      "running_reward:  -1.0788175242751954 running_cost:  292539.1929485872\n",
      "running_reward:  -1.150892457002088 running_cost:  100316.49350347463\n",
      "running_reward:  -1.0606724420625386 running_cost:  127822.47310895142\n",
      "running_reward:  -1.1985972661205595 running_cost:  128284.5356625065\n",
      "running_reward:  -2.4545776864036934 running_cost:  297199.3969313858\n",
      "running_reward:  -1.0671642447201026 running_cost:  95009.88619049358\n",
      "running_reward:  -2.351876950597811 running_cost:  507906.2212305119\n",
      "running_reward:  -0.9838826104161243 running_cost:  154753.70277900886\n",
      "running_reward:  -1.120139942554516 running_cost:  104241.60562268436\n",
      "running_reward:  -2.2944319218575497 running_cost:  531547.2624927898\n",
      "running_reward:  -1.1765788606222098 running_cost:  39853.28574081747\n",
      "running_reward:  -1.0498090252946155 running_cost:  226391.13866825897\n",
      "running_reward:  -1.262190752821551 running_cost:  226465.78911821143\n",
      "running_reward:  -2.5686031389671555 running_cost:  428415.683285227\n",
      "running_reward:  -1.1384739902436085 running_cost:  99547.8260206569\n",
      "running_reward:  -2.3948922445146645 running_cost:  320036.25482096174\n",
      "running_reward:  -0.9669974587869111 running_cost:  569666.5253449285\n",
      "running_reward:  -1.1044764853311595 running_cost:  56797.47873941005\n",
      "running_reward:  -1.1961914728704974 running_cost:  63066.81300934074\n",
      "running_reward:  -1.1443190192302597 running_cost:  83896.68733011112\n",
      "running_reward:  -1.0216636143745896 running_cost:  218204.97153911807\n",
      "running_reward:  -0.987118385272936 running_cost:  253526.18997465464\n",
      "running_reward:  -2.2730970329523155 running_cost:  484656.60340113926\n",
      "running_reward:  -0.990262310991297 running_cost:  86059.95075324144\n",
      "running_reward:  -1.0788397156498177 running_cost:  58944.178404979015\n",
      "running_reward:  -1.1476087693768475 running_cost:  75530.45334395432\n",
      "running_reward:  -1.190717477562317 running_cost:  115555.87963104363\n",
      "running_reward:  -2.413395198254951 running_cost:  618539.8819392883\n",
      "running_reward:  -1.0761609365133296 running_cost:  96850.79769248675\n",
      "running_reward:  -1.0431175031559627 running_cost:  66373.98276974823\n",
      "running_reward:  -1.1117472902209 running_cost:  76128.62323991086\n",
      "running_reward:  -0.8913484751472289 running_cost:  890886.242325839\n",
      "running_reward:  -1.3400827864189186 running_cost:  250208.66002079195\n",
      "running_reward:  -2.413465957727883 running_cost:  403298.24191640306\n",
      "running_reward:  -2.481248390508532 running_cost:  430618.496104685\n",
      "running_reward:  -0.9527447052771663 running_cost:  388404.9404724433\n",
      "running_reward:  -1.3177948918821722 running_cost:  221135.7981599\n",
      "running_reward:  -1.1912403056968557 running_cost:  44573.80458223244\n",
      "running_reward:  -1.023275950639978 running_cost:  349353.6237801386\n",
      "running_reward:  -2.4011117013785124 running_cost:  458855.06099548383\n",
      "running_reward:  -1.1568583867925712 running_cost:  96367.38569341759\n",
      "running_reward:  -0.7825894658593417 running_cost:  680798.9119249051\n",
      "running_reward:  -2.45113203447301 running_cost:  316310.5125778692\n",
      "running_reward:  -0.9983350147120503 running_cost:  351564.95524876675\n",
      "running_reward:  -1.2371425081652534 running_cost:  183670.60640717152\n",
      "running_reward:  -1.0807253735739892 running_cost:  35054.279205154424\n",
      "running_reward:  -1.0700370150813692 running_cost:  43895.05248344175\n",
      "running_reward:  -1.0968562178708774 running_cost:  205975.22793036015\n",
      "running_reward:  -2.2445147711282436 running_cost:  518927.93999994406\n",
      "running_reward:  -1.1160127363852208 running_cost:  313226.0584160356\n",
      "running_reward:  -1.3990014332592249 running_cost:  430130.50476117904\n",
      "running_reward:  -2.419323375253902 running_cost:  297874.5355986347\n",
      "running_reward:  -2.6393793452421006 running_cost:  294276.4240522675\n",
      "running_reward:  -1.1394936769552186 running_cost:  57702.465702960355\n",
      "running_reward:  -1.8912234275984374 running_cost:  713072.0353738455\n",
      "running_reward:  -1.301306021879857 running_cost:  120100.56388526545\n",
      "running_reward:  -1.1585578000916483 running_cost:  50482.409029330294\n",
      "running_reward:  -1.0674571427779256 running_cost:  348685.9753606059\n",
      "running_reward:  -1.1930887976363782 running_cost:  162307.54641714162\n",
      "running_reward:  -1.264624716648521 running_cost:  941212.4698181973\n",
      "running_reward:  -0.7164783370840663 running_cost:  689614.3762062402\n",
      "running_reward:  -1.196810746117818 running_cost:  247232.5362779414\n",
      "running_reward:  -2.654479537988305 running_cost:  335055.3213315881\n",
      "running_reward:  -2.307912129076156 running_cost:  535135.4545302776\n",
      "running_reward:  -1.3780611209907279 running_cost:  975149.881776287\n",
      "running_reward:  -1.0462106479334168 running_cost:  178343.79490271324\n",
      "running_reward:  -1.0769607030580874 running_cost:  36059.50732246826\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-4ab4309e5689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-0d1c1ef46524>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(agent, env, preprocessor, config)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_rb_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0msarsd_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mcritic_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsd_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mcritic_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mcritic_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'running_reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-435f47ee90a3>\u001b[0m in \u001b[0;36mtrain_step_critic\u001b[0;34m(self, sarsd)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mQt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'td_error'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Qt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-435f47ee90a3>\u001b[0m in \u001b[0;36mupdate_target\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbehaviour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             target.data.copy_(self.tau_soft_update * behaviour.data + \\\n\u001b[0m\u001b[1;32m    156\u001b[0m                               (1.-self.tau_soft_update)*target.data)        \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = 500_000\n",
    "\n",
    "env.reset()\n",
    "for i in tqdm(range(iterations)):\n",
    "    metrics = next(train_loop)\n",
    "    if metrics is not None:\n",
    "        train_metrics.append(metrics)\n",
    "        \n",
    "print('eps', metrics['critic']['eps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figsize(10, 7.5)\n",
    "\n",
    "\n",
    "trn_metrics = reduce_train_metrics(list_2_dict(train_metrics['critic']), ['Qt', 'Gt'])\n",
    "\n",
    "plt.plot(trn_metrics['loss'], label='loss')\n",
    "plt.plot(trn_metrics['td_error'], label='td_error')\n",
    "plt.plot(trn_metrics['Qt'], label=\"\"'Qt')\n",
    "plt.plot(trn_metrics['Gt'], label='Gt')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from madigan.utils.plotting import plot_test_metrics\n",
    "figsize(15, 10)\n",
    "reset=True\n",
    "# reset=False\n",
    "\n",
    "tst_metrics = test(agent, env, preprocessor, \n",
    "                   nsteps=50, verbose=True, reset=reset, eps=0.)\n",
    "print(tst_metrics.keys())\n",
    "\n",
    "fig, ax = plot_test_metrics(tst_metrics)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "state = {'state_dict': agent.model_t.state_dict(),\n",
    "         'ntraining_steps': i, \n",
    "        'config': config}\n",
    "torch.save(state, 'sineadder_behaviour_good.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(agent.model_t.parameters())\n",
    "print(\"biases\", params[1])\n",
    "weights = params[0].detach().cpu().numpy()\n",
    "for i in range(0, 64):    \n",
    "    plt.plot(weights[i], label=str(i))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perfect_agent = PerfectAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_metrics = test(perfect_agent, env, preprocessor, verbose=True)\n",
    "print(tst_metrics.keys())\n",
    "fig, ax = plot_test_metrics(tst_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfect_agent.get_qvals(preprocessor.current_data()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.current_data().price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
