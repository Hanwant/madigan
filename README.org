
* Madigan Tech 
** Narrative 
*** Business Model 
    *The Money Machine*
    The business model is that of an investment fund. Whether this fund is open to investors or the LLC is operated
    as a proprietary trading firm, depends on availability of resources. This outright flexibility is expected to
    allow for investors in the early phase of growth, while allowing for a restriction to new investors if scalability
    is expected to become a limiting factor. 
    
**** Stages

     Prelimanaries: [0/4] [0%]
        - [ ] Develop process for developing and testing a unique betting edge I.e "alpha"
        - [ ] Validate edge through statistical due process.
        - [ ] Validate edge through live trading (paper -> funded)
        - [ ] Accumulate months of portfolio history as a proof of concept and CV for business model.

    Milestones:
     1. 1M AUM. With an expected 2/20 fee structure, 1m in AUM would yield 20,000 in flat commission and at an expected RR of 20%,
       and 40,000 in profit commission, yielding an expected 60,000 for that year. Can take on 1 or 2 collaborators, perhaps paying
       early contributors with equity and starting to incorporate a salary from the 1M mark.

     2. 10M AUM. 2/20, E(RR) = 20%, managementFee = 200,000 performanceFee = 400,000. Starting to develop good relations with brokers,
        running distributed portfolios across a range of risk and market/asset profiles, customizable for clients. Developed a strong
        core team of researchers and engineers with sufficient computing resources as well as dedicated office space. 

     3. 100M AUM 2/20 E(RR) = 20%, managementFee = 2,000,000 performaceFee = 4,000,000. Can really start collecting bonus checks.
        After duly rewarding employess, the bonuses can be used to fund new ventures (I.e independent research, charter schools).
        With sufficient managementFees, new international offices can be opened , I.e Bombay for NSE/BSE/Asian Markets trading.

     4. 1B AUM. Economies of Scale, let the snowball roll with it's own momentum, getting bigger and faster. Developed noteriety at this 
        stage, a formidable entity in the Pacific looking to dominate Australasia and establish NZ as a player in the game. 

*** Market and Asset Class
    The market and asset choice in the early stages will be largely determined by:
      * Demonstratable edge for asset
      * Low barrier to entry I.e exchange/broker availablity, low commissions
    Following an establishment of return flow, the primary object will shift to diversification as well as scalability 
    whilst applying the now mature process of R&D -> Testing -> Deployment. Markets such as FX and assets such as ETFs
    and futures allow for scalability, with FX intrinsically serving diversification as well.
    Ideal progression of asset class:
      Crypto +==> FX +==> Commodities +==> Indices and Equities
    Ideal progression of contract class:
      Outright +==> CFDs +==> Futures +==> Options
    
*** Modelling

**** Alpha 

     The fundamental imperative of the business is to develop 'alpha' I.e returns which are not correlated to the 
     corresponding market. The approach taken is empirical rather than analytical. Although market intuition and domain 
     consideration may guide the process, the fundamental approach is that of statistical arbitrage. This is owing to the 
     systematic process entailed by the approach. Statistical Learning techniques lend themselves to forming a framework
     of tools which can serve an idea generation and testing process. The approach can be applied to different asset classes
     and the components lend themselves to composability and continual incremental improvements. A disadvantage in the early
     testing phase will be compute resources for live inference, though this can be overcome. 
     
**** Base Model
     A base model will be constructued which characterizes and encodes a univariate time-series (price + timestamp) into a
     high dimenional feature space. From this base representation, the interaction between many different (yet synchronized) 
     timeseries may be modelled. These base models can serve as core building blocks for addition of external covariates.
     Effective use of external covariate data is contingent upon creativity, luck and working capital to purchase data streams.

**** Model Types
     Neural Networks are theoretically and empirically known to possess high capability for non-linear processing of information.
     As a toolkit, NNs allow for extensive customization, inductive capabilities for naturally handling timeseries data 
     and fine-tuned control of incentive. Although they may pose a computational burden in the early phases of live-inference,
     there are numerous methods of model approximation which can greatly reduce time and space complexity. Still, other methods must
     still be considered and developed duly, so as to diversify and allow for continual expansion of general methodology.

**** Reinforcement Learning
     RL provides an end-to-end framework for applying the non-linear processing capabilities of 
     NNs natively to the problem of trading. Rather than trying to predict price (I.e regression) 
     or predict direction (classification), RL provides a way to directly optimize the problem
     at hand, finding optimal trading strategies and so is preferable to the previous approach
     of applying NNs to a prediction task and then hand crafting strategies from the predictions.
     Nevertheless, they are subject to the same constraints namely low signal-to-noise ratio in 
     financial markets.
     
     To tackle this, unique methods must be developed to enable learning while preventing over-fitting
     for the trading task.

***** DONE Establish RL  
      CLOSED: [2020-08-19 Wed 18:57] DEADLINE: <2020-08-13 Thu>
****** DONE Study RL [4/4] [100%]
        CLOSED: [2020-08-03 Mon 00:34] DEADLINE: <2020-08-02 Sun 23:59> SCHEDULED: <2020-07-30 Thu>
        :LOGBOOK:
        CLOCK: [2020-08-02 Sun 20:37]--[2020-08-03 Mon 00:34] =>  3:57
        CLOCK: [2020-08-02 Sun 18:31]--[2020-08-02 Sun 18:42] =>  0:11
        CLOCK: [2020-08-02 Sun 13:59]--[2020-08-02 Sun 17:25] =>  3:26
        CLOCK: [2020-08-02 Sun 11:52]--[2020-08-02 Sun 12:57] =>  1:05
        CLOCK: [2020-08-01 Sat 16:41]--[2020-08-01 Sat 18:45] =>  2:04
        CLOCK: [2020-08-01 Sat 13:35]--[2020-08-01 Sat 14:53] =>  1:18
        CLOCK: [2020-08-01 Sat 11:59]--[2020-08-01 Sat 13:13] =>  1:14
        CLOCK: [2020-07-31 Fri 22:29]--[2020-08-01 Sat 01:05] =>  2:36
        CLOCK: [2020-07-31 Fri 14:50]--[2020-07-31 Fri 20:39] =>  5:49
        CLOCK: [2020-07-31 Fri 12:13]--[2020-07-31 Fri 12:50] =>  0:37
        CLOCK: [2020-07-30 Thu 20:47]--[2020-07-31 Fri 00:35] =>  3:48
        CLOCK: [2020-07-30 Thu 18:58]--[2020-07-30 Thu 19:29] =>  0:31
        CLOCK: [2020-07-30 Thu 15:04]--[2020-07-30 Thu 18:15] =>  3:31
        :END:
        - [X] Complete Sutton and Barto Textbook [9/9] [100%]
          - [X] Chapter 9
          - [X] Chapter 10 
          - [X] Chapter 11 
          - [X] Chapter 12 
          - [X] Chapter 13 
          - [X] Chapter 14
          - [X] Chapter 15 
          - [X] Chapter 16 
          - [X] Chapter 17 
        - [X] Complete David Silver Lectures [4/4] [100%]
          - [X] Lecture 7
          - [X] Lecture 8
          - [X] Lecture 9
          - [X] Lecture 10
        - [X] Complete Deep RL Berkely Lectures [4/4] [100%]
          - [X] Lecture 4A
          - [X] Lecture 4B
          - [X] Lecture 5
          - [X] Lecture 6
        - [X] Skim updated DeepMind course
****** DONE Practice RL 
       CLOSED: [2020-08-19 Wed 10:52] DEADLINE: <2020-08-13 Thu 23:59> SCHEDULED: <2020-08-09 Sun>
       :LOGBOOK:
       CLOCK: [2020-08-13 Thu 19:23]--[2020-08-13 Thu 23:33] =>  4:10
       CLOCK: [2020-08-13 Thu 11:04]--[2020-08-13 Thu 13:44] =>  2:40
       CLOCK: [2020-08-04 Tue 16:06]--[2020-08-04 Tue 22:06] =>  6:00
       CLOCK: [2020-08-04 Tue 11:24]--[2020-08-04 Tue 13:31] =>  2:07
       CLOCK: [2020-08-03 Mon 12:26]--[2020-08-03 Mon 19:27] =>  7:01
       :END:
       - [-] [6/8] [75%]
         Apply Rainbow Paper + PG methods (I.e PPO, DPG) to choice of Atari and continuous problem.
         In the process, develop the intuition for using these algorithms and
         the framework for automated (I.e plug n play) composition of different RL components

         - [X] Set up env, agents and test/train scripts for logging and automated experiments
         - [X] Double-DQN
         - [X] Dueling Networks
         - [X] IQN
         - [X] FQF
         - [ ] Prioritized replay
         - [X] Multi-step Learning
         - [ ] Noisy Nets


***** TODO Establish Trading RL environment [1/8] [12%]
      SCHEDULED: <2020-08-22 Sat> DEADLINE: <2020-09-28 Mon>
      :LOGBOOK:
      CLOCK: [2020-09-08 Tue 11:10]--[2020-09-08 Tue 13:11] =>  2:01
      CLOCK: [2020-09-02 Wed 17:29]--[2020-09-02 Wed 20:41] =>  3:12
      CLOCK: [2020-08-31 Mon 20:26]--[2020-08-31 Mon 22:55] =>  2:29
      CLOCK: [2020-08-31 Mon 13:03]--[2020-08-31 Mon 17:12] =>  4:09
      CLOCK: [2020-08-30 Sun 16:35]--[2020-08-30 Sun 22:32] =>  5:57
      CLOCK: [2020-08-30 Sun 11:24]--[2020-08-30 Sun 12:04] =>  0:40
      CLOCK: [2020-08-29 Sat 13:52]--[2020-08-29 Sat 21:27] =>  7:35
      CLOCK: [2020-08-27 Thu 17:02]--[2020-08-27 Thu 20:02] =>  3:00
      CLOCK: [2020-08-27 Thu 15:28]--[2020-08-27 Thu 16:30] =>  1:02
      CLOCK: [2020-08-26 Wed 18:34]--[2020-08-26 Wed 23:13] =>  4:39
      CLOCK: [2020-08-26 Wed 13:30]--[2020-08-26 Wed 16:34] =>  3:04
      CLOCK: [2020-08-25 Tue 20:30]--[2020-08-25 Tue 23:30] =>  3:00
      CLOCK: [2020-08-25 Tue 16:21]--[2020-08-25 Tue 19:30] =>  3:09
      :END:

       ALL Trading stategies fit into the RL framework. 
       env state -> model -> action -> env  reward + new state 
       This is model independent, I.e may be decision trees or manual crossovers etc

       Objectives
       Rewrite trading environment from the ground up (+ integrate arena into RL setup.)
       As with arena, generalize to multi-dimensional inputs I.e Multiple assets
       Make visualizing + monitoring tools in Qt (I.e pyQt and pyqtgrah)
       Enable distributed computation and networking (I.e data feeds) early.
       Don't plan in advance too much, just keep these things in mind.
       Build ground up, as you need to not waste time.
       Work end-to-end now, instead of over-compartmentalising the components
       Work on the direct task of executing profitable trading stategies.

       - [X] Setup Synth environments  [5/5]
         - [X] Formalize principled action space
         - [X] Data-Generating Processes
         - [X] mean-reversion I.e sine
         - [X] Make base viz tools - Qt
         - [X] Accounting
       - [ ] Rewrite main logic components of arena in c++ for env use in RL
       - [ ] Establish tests
       - [-] Make and train agents to play the synth [2/4] [50%]
         - [X] Extend Control Panel for training
         - [X] DQN
         - [ ] PG
         - [ ] Make Scalable [0/3] [0%]
           - [ ] multiprocessing
           - [ ] zmq 
           - [ ] ray
       - [ ] Robust pre-processing
       - [ ] Test on more data generating processes and implement some Sota [0/6] [0%]
         - [ ] ad-hoc - I.e trended
         - [ ] ad-hoc - Multi-Asset relationships - statistical arbitrage
         - [ ] Experimental - I.e via representations from from VAE/GAN (I.e as in world model)
         - [ ] Extensions I.e Actor-Critic, PPO
         - [ ] Model-Based I.e VAE/GAN
         - [ ] Analytical processes I.e Ornstein-Uhlenbeck
       - [ ] Robust pre-processing
       - [ ] Train on idealized data 
       - [ ] Train/Test on real data


***** Swarm Learning
      Taking inspiration from random forests, weak RL agents (I.e small NNs) can be trained on semantic subsets of
      data I.e trending/mean-reverting periods. The overall strategy would consist of a "swarm" of these agents 
      being queried for their policies  when their environment conditions are met. They are only ever trained on
      specific subsets of data akin to regimes as well as any reasonable clustering/segmentation conditions.
      Being Weak learners, they prevent over-fitting and by specializing to particular market conditions, they are able
      to work in self-similar environments where noise may be reduced, relative to the full scale problem. 
      The "Swarm" consists of an ensemble of specialized agents, being queried when their specialized environment conditions
      are met. Given their respective specializations, the performance (PnL curves) of the agents themselves provides 
      a characterization of market regime as well as a natural method to handle under-performance by providing an entry point
      into model interpretation (regime + inferred approach of weak agent)
     
      Key Components:
      - Criteria for segmentation of data. (Random Forests do this randomly)
      - Features provided to each agent.   (Random forests do this randomly)
      - Choice of agent.                   (Random forests typically use decision trees)

*** Data Processing
*** Backtesting
** Engineering [0/3] [0%]
*** TODO Data [0/4] [0%] 
**** TODO Pytorch Data Handling [0/2] [0%]
      Datasets, Samplers and Dataloaders are to be flexible in that many scenarios can be easily composed.
      I.e combinations of different assets, time frames, inner joins etc.
      - [ ] Dataset Cache - works with multiprocessing and possibly asynchronous.
      - [ ] Native Multiasset handling throughout dataset, sampler and dataloader classes
**** TODO Data Download Apis
**** TODO Database format
**** TODO Read and Write Api
*** TODO Pre-Processing [3/5] [60%]
**** DONE Rollers [5/5] [100%] 
     CLOSED: [2020-07-02 Thu 19:58] DEADLINE: <2020-06-26 Fri> SCHEDULED: <2020-06-15 Mon>
     :LOGBOOK:
     CLOCK: [2020-06-20 Sat 13:58]--[2020-06-20 Sat 14:23] =>  0:25
     :END:
     Currently written in cython and nearly finished though considering rewriting in C++ and using cython 
     to wrap - this allows for better native extensibility for future revisions of the preprocessing algorithms. 
     This will also serve as an educational exercise for C++.
     C++ Classes will be designed for extensibility and imperative composability.

     - [X] Read/Write HDF + Plot using GNUplot or similar
     - [X] Consolidate C++ -> Python + testing workflow
     - [X] Write simple baseline rollers 
     - [X] Recreate the Cython Classes in C++ and test 
     - [X] Wrap the C++ classes in Python with a nice API and docs
**** DONE Optimize C++ code + compiler directives
     CLOSED: [2020-07-08 Wed 15:58]
     Instead of Eigen, try boost for internal arrays (esp now that converting to/from numpy is trivial)
     Also, Try implementing own matrix class, appropriately strided for vectorization.
     Matrix Class Needs:
     - shared_ptr to dynamically allocated vector - automatic garbage collection
     - rows() and cols() methods
     - resize() method
     - indexing - either via () or [] operators
     Ended up using EIGEN but found the reason for it being slow - improper passing of array from numpy to 
     C++ causing copying at every loop (every time it is accessed in the inner loop

     All that needed to be added:
     #+BEGIN_SRC c++
     zoneBoolType zones(zones_memory.rows() + zones_in.rows(), zones_in.cols());
     zones << zones_memory, zones_in;
     #+END_SRC

**** DONE Add Tests And Implement RollerY [9/11] [81%]
     CLOSED: [2020-07-13 Mon 17:34] DEADLINE: <2020-07-13 Mon 23:59>
     :LOGBOOK:
     CLOCK: [2020-07-13 Mon 19:30]--[2020-07-13 Mon 22:54] =>  3:24
     CLOCK: [2020-07-13 Mon 15:44]--[2020-07-13 Mon 17:32] =>  1:48
     CLOCK: [2020-07-13 Mon 14:22]--[2020-07-13 Mon 14:52] =>  0:30
     CLOCK: [2020-07-13 Mon 11:00]--[2020-07-13 Mon 12:28] =>  1:28
     CLOCK: [2020-07-12 Sun 23:40]--[2020-07-13 Mon 01:29] =>  1:49
     CLOCK: [2020-07-12 Sun 20:37]--[2020-07-12 Sun 22:11] =>  1:34
     CLOCK: [2020-07-12 Sun 19:13]--[2020-07-12 Sun 20:11] =>  0:58
     CLOCK: [2020-07-12 Sun 11:00]--[2020-07-12 Sun 12:00] =>  1:00
     CLOCK: [2020-07-11 Sat 20:53]--[2020-07-11 Sat 22:54] =>  2:01
     CLOCK: [2020-07-10 Fri 22:00]--[2020-07-11 Sat 00:58] =>  2:58
     CLOCK: [2020-07-10 Fri 17:02]--[2020-07-10 Fri 19:29] =>  2:27
     CLOCK: [2020-07-10 Fri 16:07]--[2020-07-10 Fri 16:50] =>  0:43
     CLOCK: [2020-07-10 Fri 11:10]--[2020-07-10 Fri 12:46] =>  1:36
     :END:
     - [X] Add Tests for Continuous Timeframes with Pandas as Reference
       C++ Rollers Are 1000< FASTER than Pandas !! - Mostly due to custom apply(lambda x: f(x)) loops
     - [X] Confirm Positive Tests
     - [X] Add Tests for Discrete Timeframes with Pandas as Reference
     +- [ ] Confirm Positive Tests+
       ---------------------------------------------------------- 
       SKIPPING DISCRETE TIMEFRAMES FOR NOW AS IT IS NOT CRITICAL 
       AND ROLLERY + SAMPLING IS MORE IMPORTANT 
       ---------------------------------------------------------- 
     - [X] Translate + set pandas reference for testing RollerY
     - [X] Reformulate RollerY to separate functions for outCont and outLabel
       For outCont - just shift xFeats instead of recalculating!
     - [X] Validate via Test.
     - [X] Test Sequential Stateful Behaviour for RollerX 
     - [X] Test Sequential Stateful Behaviour for RollerY.shift()
     - [X] Check outLabels are looking alright - numpy version looks good
     - [ ] Test logic for RollerY.roll()
     - [ ] Test Sequential Stateful Behaviour for RollerY.roll()


     For YLabels:
      NUMPY IMPLEMENTATION IS ALREADY VECTORIZED AND FAST
      NO CACHE NEEDED AS yFeats are used.
      SUFFICIENT FOR RESEARCH PURPOSES.

     For now, can focus on things like gramian matrices and matrix profiles, the latter for which
     there are already efficient libraries available - stumpy has incremental impl for streaming.
     
     MAIN IMPERATIVE
     Crucial to get preprocessing down for more research and exploration!

**** TODO Clean up Rollers and Test Templating for function routing
     
**** TODO Add more time series features
     :LOGBOOK:
     CLOCK: [2020-07-09 Thu 22:10]--[2020-07-09 Thu 22:53] =>  0:43
     CLOCK: [2020-07-09 Thu 20:37]--[2020-07-09 Thu 20:37] =>  0:00
     CLOCK: [2020-07-09 Thu 18:22]--[2020-07-09 Thu 20:37] =>  2:15
     :END:
     ONGOING DEVELOPMENT

     These have varying importance's and so the task will be closed 
     when the pressing ones have been completed. The number and % bars
     will still show the number of tasks uncompleted, for future reopening.
     Use Template Meta-Programming to optimize various combinations of function choices
    
***** TODO Pre-Requisite: Solid Foundation in Calculus [0/4] [0%]
      - [ ] Calculus 1 - KA
      - [ ] Calculus 2 - KA
      - [ ] Differential Equations - KA
      - [ ] Multi-variable Calculus - KA
***** TODO High Priority [1/6] [16%]
      SCHEDULED: <2020-08-14 Wed>
      - [ ] Templated Function Routing
      - [ ] Time Sampling 
      - [X] HL Sampling
      - [ ] Return Sampling
      - [ ] Volatility Sampling
      - [ ] Wavelet Smoothing/transformations

***** TODO Mid Priority [0/6] [0%]
      - [ ] Templating for function routing + composition
      - [ ] Multiprocessing 
      - [ ] Stacked multi timeframe Data
      - [ ] Kalman Filters
      - [ ] Co-integration
        
*** TODO Model Implementation [0/2] [0%]
    ONGOING DEVELOPMENT

    Models must be implemented with enough components built ground-up so that they can be customized and 
    experimented with. 
    Current models include:
    - TransformerBase 
    - LSTM
    - CNN
**** TODO Model Components [0/4] [0%] 
     ONGOING DEVELOPMENT

     - [ ] Time encoding
     - [ ] GRN with gated linear unit
     - [ ] Variable Selection Network 
     - [ ] (Log) Sparse Attention 
     - [ ] 3d Attention Module for cross-asset attention
**** TODO Models to Implement [0/4] [0%]
     ONGOING DEVELOPMENT 

     - [ ] Transformer with CNNs for local context processing
     - [ ] Temporal Fusion Transformer TFT - for 'Regime' adaptibility from GRN units + interperability
       from variable selection networks and potentially shared Wv for Multi-Head Attention. 
     - [ ] Graph Network for native currency exchange representation
*** Training 
**** NN Direct Training
     Training classes offer a simple api and handle saving/loading logs/models etc. They also provide an
     interface to the training dashboard via being run as a server which will listen to training commands sent
     from the dashboard gui via zmq sockets.
     #+begin_src python
     trainer.run() # Server mode, accepting training commands via zmq socket
     #+end_src

**** TODO RL 
     Apply to trading using a standard environment - integrate with ARENA.
     When done, move on to implement swarm learning
*** Swarm Learning
    Given that a pre-processing pipeline is established to extract suitable features and base models
    have been implemented, swarm learning can be implemented.
   
    
** Timeline
*** August - Setup
    *Modelling and Feature Engineering*
    - Establish RL. Study + practice applying to some problems I.e Chess before trying market env. 
    - Establish RL within Arena environment by end of month. Set up appropriate methods I.e policy gradients, NNs, Evolutionary decision trees.
    - Develop feature engineering pipeline I.E Finish Rollers so they are ready to use with heterogeneous datasets.
*** September - Experiments
    *Modelling Execution; Edge*
    - Gather crypto data
    - Market Data Segmentation 
    - Feature Construction I.e Wavelets/Fourier, matrix profile
    - Exploratory Analysis
    - Establish SWARN techniques
    - Run SWARM with distributed + parallel deployment
    - Identify successful patterns of features + models
    - Establish training edge
*** October - Deploy
    *Production System*
    - ReImplement Arena (some in C++) and validate training environments
    - Validate edge
    - Arena broker integration - start with crypto exchanges
    - Develop arena to be production ready I.e run in server with Logs + dashboard UI
    - Consider deployment solutions I.e Docker + Kubernetes
    - Live Test
*** November - Monitor
    *Monitor, debug and consolidate process*
    - Monitor live performance
    - Test on FX using IB demo api
    - Establish return stream on scalable forex strategies
*** December - Monetize
    *Business Plan*
    - Develop business plan
    - Make performance reports
    - Technical reports
    - Business proposal for VC investors
    - Find collaborators
    - Funding

** Tasks

*** Daily



*** Weekly


*** Monthly

